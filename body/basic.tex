\chapter{相关算法基础}
   本文中提出的算法使用了大量最新强化学习算法中的思想，因此有必要介绍强化学习中常用的概念和现有强化学习算法的思想。

    \section{强化学习基础}
    在强化学习中，我们对智能体如何与环境交互感兴趣，这涉及到4个概念：模型、状态、动作和奖励\cite{sutton1988reinforcement}。
    由与智能体和环境相关的可观测量构成了智能体的状态向量$s$，它表示了智能体当前在环境中所处的状态。
    每个状态从状态空间$\mathcal S$中取值，$\mathcal S$包含了所有智能体和环境可以取到的状态。一个智能体可以在某一时刻做出动作$a$并因此转移到下一个状态。
    所有的动作从动作空间$\mathcal A$中取值。如果完全已知一个环境的模型，且已知智能体的当前状态，就可以知道智能体做出任一动作的效果。当智能体做出某一个动作之后，根据当前的任务和环境可以得到一个奖励$r$，它表示智能体从环境中得到的反馈，并从奖励空间$\mathcal R$中取值。
    状态转换、动作和它导致的奖励构成了一个轨迹。如果用$S_t$，$A_t$，$R_t$分别表示在时刻$t$下智能体的状态、动作和奖励，那么轨迹就可以表示为一个序列$S_t,A_t,R_{t+1},S_{t+1},\cdots,S_T$，其中$T$是关心的整个片断结束的时刻。
    策略$\pi(a|s)=\mathbb P[A_t=a|S_t=s]$是智能体在当前状态$S_t=s$时采取动作$A_t=a$的条件概率。
    每个状态都有一个关联的值表示当前状态在特定任务下的价值，用状态价值函数$V_\pi:\mathcal S\to\mathbb R$来表示从一个状态到这个值的映射。
    状态价值函数实际上表示了智能体使用当前策略$\pi$来选择动作，在可能获得奖励的多少。
    类似地，用动作价值函数$Q_\pi:\mathcal S\times\mathcal A\to\mathbb R$表示智能体在给定当前状态和动作后，随后一直使用策略$\pi$来选择动作，在未来获得奖励的多少。
    在在实际算法中，为了防止对价值函数的拟合发散，需要给定一个折扣系数$\gamma\in[0,1]$来减少较远未来获得的奖励对当前价值函数的影响。
    强化学习的最终目标实际上为了最大化累积奖励：
    $$G_t=\sum_{k=0}^\infty \gamma^k R_{t+k+1}$$
    有了累积奖励的定义后，状态价值函数$V_\pi$就可以写成在未来使用策略$\pi$的期望的累积奖励：
    $$V_\pi(s)=\mathbb E_\pi[G_t|S_t=s]$$
    类似地，动作价值函数可以写成：
    $$Q_\pi(s,a)=\mathbb E_\pi[G_t|S_t=s,A_t=a]$$

    \section{TD3算法}\label{td3sec}
    在通常的演员－评论家模式的强化学习算法中\cite{konda2002actor}，演员网络被用于拟合最优的确定性策略$\mu^*:\mathcal S\to\mathcal A$，评论家网络被用于拟合最优的动作价值函数$Q^*:\mathcal S\times \mathcal A\to \mathbb R$。在给定一个迁移$(S_t,A_t,R_t,S_{t+1})$，和现有的演员网络$\mu$和评论家网络$Q$之后，根据如下损失函数对网络参数进行优化：
    $$L_Q = \mathbb E[(R_t + \gamma Q(S_{t+1},\mu(S_{t+1})) - Q(S_t, A_t))^2]$$
    $$L_\mu = \mathbb E[-Q(S_t, \mu(S_t))]$$

    但是由于在上述函数拟合过程中，评论家网络往往倾向于输出更高的动作值或状态价值。
    根据TD3算法\cite{DBLP:journals/corr/abs-1802-09477}，可以使用两个评论家网络$Q_1,Q_2:\mathcal S\times\mathcal A\to \mathbb R$。
    这两个评论家网络同时对未来时刻$t+1$时刻的动作和状态进行评估并分别输出价值$Q_1(S_{t+1},A_{t+1})$和$Q_2(S_{t+1},A_{t+1})$。
    取这两个值的最小值作为对未来时刻$t+1$的价值预测，即可得到修正后的评论家网络的损失函数：
    $$L_{Q_1} = \mathbb E[(R_t + \gamma \min\{ Q_1(S_{t+1},\mu(S_{t+1})), Q_2(S_{t+1},\mu(S_{t+1})) \} - Q_1(S_t, A_t))^2]$$
    $$L_{Q_2} = \mathbb E[(R_t + \gamma \min\{ Q_1(S_{t+1},\mu(S_{t+1})), Q_2(S_{t+1},\mu(S_{t+1})) \} - Q_2(S_t, A_t))^2]$$
    在对演员网络进行优化时，只使用评论家网络$Q_1$：
    $$L_\mu = \mathbb E[-Q_1(S_t, \mu(S_t))]$$

    为了防止在训练评论家网络时出现发散或损失不稳定的情况，可以对每个网络引入一个靶网络。
    对每个靶网络权重，不使用损失函数对其进行优化，而是使用原网络和靶网络权重的指数滑动平均进行更新。
    对上述网络$\mu, Q_1, Q_2$，有对应的靶网络$\mu',Q_1',Q_2'$。
    在训练刚开始时，保持原网络和靶网络权重相同。
    在之后的训练过程中，使用上述损失函数对$\mu, Q_1, Q_2$进行更新，而使用如下公式分别对靶网络的权重$W_{\mu'},W_{Q_1'}, W_{Q_2'}$进行更新：
    $$W_{\mu'} = \tau W_\mu + (1-\tau) W_{\mu'}$$
    $$W_{Q_1'} = \tau W_{Q_1} + (1-\tau) W_{Q_1'}$$
    $$W_{Q_2'} = \tau W_{Q_2} + (1-\tau) W_{Q_2'}$$
    其中$\tau$叫做polyak系数，它控制着每次权重更新的多少。
    TD3算法工作原理如图\ref{td3}所示，图中的替换使用了上述指数滑动平均方法，$L_\mu$表示演员网络的损失，$L_{Q_1}$和$L_{Q_2}$分别表示根据上述公式计算出的评论家网络1和评论家网络2的损失。
    \input{body/td3}

    \section{事后经验重放算法}\label{HERsec}
    事后经验重放（HER）算法\cite{DBLP:journals/corr/AndrychowiczWRS17}是一个用于稀疏奖励的强化学习算法，它主要是为了解决奖励过少导致的训练速度过慢的问题。
    
    事后经验重放要求状态向量$s$由观测向量$o$，已完成目标$g^a$，期望目标$g^d$构成，即$s=(o, g^a, g^d)$。
    在智能体与环境交互的过程中，每当获得一个迁移$(s_t, a_t, r_t, s_{t+1})$后，就把它放入重放经验缓冲（replay buffer）中。
    通常情况下，在训练时，从重放经验缓冲中直接取出一个个迁移并根据这些数据对网络进行训练。

    而在事后经验重放算法中，为了增加成功奖励的个数，对每个取出的属于片段$e_i$的迁移$j$，也即$(s_j, a_j, r_j, s_{j+1})$，以一定概率$p_{future}$取出在它之后的一个迁移$k$，也即$(s_k, a_k, r_k, s_{k+1})$，其中有$j\leq k\leq T$，$T$是片段$e_i$的长度，并把迁移$j$的状态向量$s_j=(o_j,g^a_j, g^d_j), s_{j+1}=(o_{j+1},g^a_{j+1}, g^d_{j+1})$中的期望目标$g^d_{j}, g^d_{j+1}$替换为未来在$k$时刻已完成的目标$g^a_{k}$，最后根据新的状态对这些替换过的迁移重新计算奖励。

    在上述算法中，概率$p_{future}=1-\frac{1}{1+K_{replay}}$，其中$K_{replay}$是一个超参数，它决定了在经验重放时替换过的迁移所占比例的大小。

    \section{基于局部敏感哈希和计数的探索奖励}\label{LSHsec}
    在开放任务中，由于奖励是稀疏的，智能体在自由探索过程中很难通过成功的奖励获知有关完成任务目标的信息，为了解决这一问题，需要引入任务无关的内部奖励来帮助智能体更快地探索有意义的状态，防止智能体对访问过的状态进行多次重复访问。

    基于局部敏感哈希和计数的探索奖励\cite{DBLP:journals/corr/TangHFSCDSTA16}使用一个局部敏感哈希函数对状态空间进行离散化，并对访问过的状态对应的哈希值进行计数，并通过此计数值计算出的奖励鼓励智能体访问之前未访问过的状态。

    给定一个状态向量$s\in \mathcal S$和一个动作向量$a\in \mathcal A$，进行链接可以得到新向量$x=(s,a)$，接着使用SimHash函数计算它的哈希值：
    $$\phi(s)=\mathrm{sgn}(A x)\in\{-1,1\}^k$$
    其中$A$是随机生成的矩阵，满足：
    $$A\in \mathbb R^{k\times \mathrm{dim}(\mathcal S)}, A_{ij}\sim \mathcal N(0,1)$$ 
    $A$在算法的初始化过程中随机生成，并在之后的训练过程中保持不变，而$k$是一个超参数，它的大小决定了哈希向量的长度，因此决定了离散化后的状态空间的粒度。

    每当智能体到达一个状态$s$后，都可以根据上述哈希函数计算哈希值$\phi(s)$，并使用一个字典对此哈希值进行计数，即：
    $$freq[\phi(s)] += 1$$
    因此字典$freq$就记录了访问具有同样局部敏感哈希值的相似状态的频率，并可计算奖励：
    $$ reward_{lsh}(s)=\frac{1}{\sqrt{freq[\phi(s)]}}$$ 


    \section{基于正向动力学预测的探索奖励}
    每当智能体获得一个迁移$(s_t, a_t, r_t, s_{t+1})$，可用于进行正向动力学预测的信息就更多了，这种信息蕴含在$(s_t,a_t)\mapsto s_{t+1}$的映射中，它反映了环境的动力学性质。
    
    基于正向动力学预测的探索奖励\cite{DBLP:journals/corr/StadieLA15}使用一个神经网络来拟合这种正向的动力学过程，即在理想情况下，预测模型$f:\mathcal S\times \mathcal A\to \mathcal S$应当可以正确地预测未来的状态：$f(s_t, a_t) = s_{t+1}$。

    对于经常访问到的简单状态和动作，正向动力学预测模型更有可能输出正确的预测值，而对于复杂未知的状态和动作，正向动力学预测模型则更有可能出错。
    这意味着可以把正向动力学预测模型的损失作为探索奖励来鼓励智能体探索更难被模型拟合的动力学过程。

    给定一个迁移$(s_t, a_t, r_t, s_{t+1})$和一个正向动力学预测模型$f$，它的损失定义为：
    $$ L_{phy} = ||f(s_t, a_t) - s_{t+1}||^2$$ 

    根据以上分析，可以把这个损失值当作内部奖励提供给智能体，于是有奖励函数：
    $$ reward_{phy}(s_t, a_t, s_{t+1}) =  L_{phy}(s_t, a_t, s_{t+1}) $$ 
