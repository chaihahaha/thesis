\chapter{相关算法基础}
   本文中提出的算法使用了大量最新强化学习算法中的思想，因此有必要介绍强化学习中常用的概念和现有强化学习算法的思想。

    \section{强化学习基础}
    在强化学习中，我们对智能体如何与环境交互感兴趣，这涉及到4个概念：模型、状态、动作和奖励\cite{sutton1988reinforcement}。
    由与智能体和环境相关的可观测量构成了智能体的状态向量$s$，它表示了智能体当前在环境中所处的状态。
    每个状态从状态空间$\mathcal S$中取值，$\mathcal S$包含了所有智能体和环境可以取到的状态。一个智能体可以在某一时刻做出动作$a$并因此转移到下一个状态。
    所有的动作从动作空间$\mathcal A$中取值。如果完全已知一个环境的模型，且已知智能体的当前状态，就可以知道智能体做出任一动作的效果。当智能体做出某一个动作之后，根据当前的任务和环境可以得到一个奖励$r$，它表示智能体从环境中得到的反馈，并从奖励空间$\mathcal R$中取值。
    状态转换、动作和它导致的奖励构成了一个轨迹。如果用$S_t$，$A_t$，$R_t$分别表示在时刻$t$下智能体的状态、动作和奖励，那么轨迹就可以表示为一个序列$S_t,A_t,R_{t+1},S_{t+1},\cdots,S_T$，其中$T$是关心的整个片断结束的时刻。
    策略$\pi(a|s)=\mathbb P[A_t=a|S_t=s]$是智能体在当前状态$S_t=s$时采取动作$A_t=a$的条件概率。
    每个状态都有一个关联的值表示当前状态在特定任务下的好坏，用状态值函数$V_\pi:\mathcal S\to\mathbb R$来表示从一个状态到这个值的映射。
    状态值函数实际上表示了智能体使用当前策略$\pi$来选择动作，在可能获得奖励的多少。
    类似地，用动作值函数$Q_\pi:\mathcal S\times\mathcal A\to\mathbb R$表示智能体在给定当前状态和动作后，随后一直使用策略$\pi$来选择动作，在未来获得奖励的多少。
    在在实际算法中，为了防止对值函数的拟合发散，需要给定一个折扣系数$\gamma\in[0,1]$来减少较远未来获得的奖励对当前值函数的影响。
    强化学习的最终目标实际上为了最大化累积奖励：
    $$G_t=\sum_{k=0}^\infty \gamma^k R_{t+k+1}$$
    有了累积奖励的定义后，状态值函数$V_\pi$就可以写成在未来使用策略$\pi$的期望的累积奖励：
    $$V_\pi(s)=\mathbb E_\pi[G_t|S_t=s]$$
    类似地，动作值函数可以写成：
    $$Q_\pi(s,a)=\mathbb E_\pi[G_t|S_t=s,A_t=a]$$

    \section{TD3算法}

    \section{HER算法}

    \section{基于局部敏感哈希和计数的探索奖励}

    \section{基于正向动力学预测的探索奖励}
