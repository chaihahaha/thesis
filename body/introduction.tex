\chapter{绪论}

    \section{选题背景及研究意义}
    
        \subsection{选题背景}
        一直以来，强化学习和机器人学都是人工智能研究中的热门领域。在2008年，Deepmind团队基于深度强化学习研发的围棋人工智能系统AlphaGo Zero在零知识自我对弈的情况下在几天之内超越了旧的系统AlphaGo，而AlphaGo曾击败了围棋领域中世界公认的专家柯洁等人\cite{silver2018general}。这项研究使越来越多的人们开始关注人工智能领域，并使得强化学习成为研究热点。事实上，强化学习已经成为最热门的研究领域之一，并在自动控制、运筹学、机器人学、游戏智能体和无人驾驶等领域中获得了广泛的应用\cite{dosovitskiy2017carla}。在这些领域中，机器人学是和前沿强化学习算法关系最密切的领域之一。传统的机器人如机械臂、四足机器人等，可以用强化学习训练得到的智能体进行控制，并在与环境交互过程中根据环境反馈使策略得到进一步的优化。

        然而，由于机器人设计和制造的成本较高，通用的多关节机器人通常非常昂贵。而且机器人通常容易在强化学习中的各种随机探索中受到损坏，并导致控制系统和智能体策略在训练过程中出现错误。因此在实际的机器人上进行所有强化学习训练是不现实的\cite{toussaint2018differentiable, todorov2012mujoco}。为了避免这个问题，可以使用物理仿真引擎对机器人和环境进行建模，并在实际测试智能体之前先在仿真环境中对智能体进行训练。幸运的是，随着强化学习仿真需求的增加，越来越多的针对机器人的仿真环境开始出现，在这些仿真环境中，可以像在真实环境中一样控制机器人的关节、调节各种参数，或获得传感器数据等等，并可以做到在真实环境中难以做到的设定复杂的稀疏奖励、获取碰撞次数、和改变环境的物理参数等操作\cite{savva2019habitat}。

        虽然已经有大量的软件系统可以用于在一个环境建模完全精确的情形下解决一个良定义的任务\cite{toussaint2018differentiable}，如何让智能体在面对未知的新环境和未知的新任务后能够有效泛化之前学习到的策略仍然是一个未完全解决的问题。人类可以在陌生的环境中用很少次数的探索自然地掌握大量有效信息，还可以利用已有经验对大量物体进行分类、提高对物理实体运动的预测能力，或创新性地设计工具解决问题。由于人们对大脑的工作原理仍然知之甚少，这个过程通常很难被数值化为一个单一的奖励函数或强化学习算法。

        本课题致力于解决上述问题，即设计算法从而可以训练出能在任务奖励未知的环境中进行探索，获取环境信息，学习基本策略，并在任务确定后快速调节旧策略以适应新任务的智能体。为了实现这个目标，需要利用现有的开源物理仿真引擎、前沿的强化学习算法和具有强大函数拟合能力的深度神经网络来设计新算法。
        
        \subsection{研究意义}

        机器人控制对于工业制造有着重要的意义，在工厂流水线上，机械臂常常被设计为只能完成单个简单任务，或需要工人远程控制。虽然它们已经极大地提高了生产效率，减少了对工人们生命财产安全的威胁，但是机械臂由于价格昂贵，仅仅用于单一任务会造成极大的资源浪费。

        本课题提出的算法有希望训练出可以在对未知任务奖励的环境进行充分探索之后，快速适应多种不同任务的机器人智能体，从而扩大现有强化学习算法的适用范围，解决更复杂的控制任务，增强机器人智能体泛化策略的能力。

        此外，本课题还可以加深对现有强化学习算法在机器人控制中应用价值的理解，可以通过机器人仿真和控制帮助提前发现在应用算法到实际机器人控制时可能出现的问题，可以通过设计和调整深度神经网络进一步了解不同结构的神经网络对智能体性能的影响。
    
    \section{国内外研究进展}

    国内外已经有了很多关于让机器人智能体使用工具和泛化已学习的策略到新任务的研究。其中关注工具使用、无监督兴趣导向的探索、模仿学习和自驱动的主动学习与本课题有关。
        \subsection{基于引导视觉预见的工具使用}
        引导视觉预见\cite{xie2019improvisation}可以使机械臂智能体从人类演示中学习并泛化学习到的能力以在不同的环境中使用工具。这种方法包含动作提案模型和预测模型。其中动作提案模型使用演示动作数据来训练一个自回归的长短时记忆网络模型来根据图像传感器拍摄到的图像数据生成动作。预测模型是基于卷积长短时记忆网络的\cite{shi2015convolutional}。预测模型被用于对物理实体的运动进行物理预测，并可以被用于筛选不能完成指定目标的动作序列。训练过程分为基于演示的模仿学习和利用握爪反射的随机自动训练。在测试过程中，指定的目标被定义为像素移动，预测模型输出的像素位置和真实像素位置的距离被用于评估动作好坏。在指定任务后，动作序列从动作提案模型中采样，并使用交叉熵方法结合预测模型进行优化。实验表明，此种方法可以比单纯模仿学习在新环境中获得更好的泛化性能。

        \subsection{无监督兴趣导向探索}
        引导视觉预见需要有人类使用工具的演示数据才能正常工作，而对于更一般的环境，人类的演示数据可能是无法获得的，此时智能体应当可以在没有指定任务的情况下在环境中探索。为了解决这个问题，一个兴趣导向的探索方法被提出了\cite{laversanne-finot2018curiosity}。这个方法结合了解纠缠目标空间的变分自编码机（VAE）和兴趣导向的IMGEP（Intrinsically Motivated Goal Exploration Process）方法\cite{DBLP:journals/corr/abs-1708-02190}。在目标空间被给定后，IMGEP框架下的智能体会倾向于选择更有可能增加竞争力的目标。因为不像通常的强化学习算法一样在给定单一目标后做训练，而是无监督地选择目标进行训练，因此这是一个元策略算法。在探索过程中，智能体会学习到被$\beta$-VAE解纠缠后的观测，因此智能体可以分离地探索不同的物体。在实验中，此种方法获得了比单纯IMGEP方法更大的探索率。

        \subsection{模仿学习}
        在需要使用工具求解的开放任务中，往往有着稀疏奖励，随机探索很难刚好完成一个完整的动作序列，最终成功地使用工具完成任务，并获得奖励。
        模仿学习通过人类的专家策略，可以极大地加速这种随机探索过程。通过人类提供的演示数据，智能体应当能够学习到更好的探索策略，增大获得奖励的概率。
        相关研究表明，使用常规的强化学习算法和运动剪辑数据集，智能体可以学会组合学到的不同的技能，并用于求解多种任务\cite{peng2018deepmimic}。不仅如此，智能体也可以从视频中学习到一些技能\cite{peng2019sfv}。这意味着现有模仿学习方法可以利用网络中大量的视频数据进行学习。

        \subsection{自驱动的主动学习}
        在开放任务求解中，主动学习技术也可以被使用。为了主动学习预测物理环境的能力，可以使用带策略的循环Q网络来减少未知物理性质的熵\cite{li2019active}。
        在机器人学中，逆动力学模型对于稳健控制非常重要。一种主动学习方法使用竞争力来选择目标并在高维连续空间中学习各种技能，并用于机器人控制\cite{baranes2013active}。仿真结果表明这种方法能够帮助机器人智能体探索到随机策略难以探索到的区域。

    \section{研究内容和方法}

        \subsection{研究内容}

        \subsection{研究方法}

