\chapter{绪论}

    \section{选题背景及研究意义}
    
        \subsection{选题背景}
        一直以来，强化学习和机器人学都是人工智能研究中的热门领域。在2008年，Deepmind团队基于深度强化学习研发的围棋人工智能系统AlphaGo Zero在零知识自我对弈的情况下在几天之内超越了旧的系统AlphaGo，而AlphaGo曾击败了围棋领域中世界公认的专家柯洁等人\cite{silver2018a}。Deepmind的研究激发了各个研究领域对于强化学习技术的兴趣。事实上，强化学习已经成为最热门的研究领域之一，并在自动控制、运筹学、机器人学、游戏智能体和无人驾驶等领域中获得了广泛的应用\cite{dosovitskiy2017carla}。在这些领域中，机器人学是和前沿强化学习算法关系最密切的领域之一。传统的机器人如机械臂、四足机器人等，可以用强化学习训练得到的智能体进行控制，并在与环境交互过程中根据环境反馈使策略得到进一步的优化。

        然而，由于机器人设计和制造的成本较高，通用的多关节机器人通常非常昂贵。而且机器人通常容易在强化学习中的各种随机探索中受到损坏，并导致控制系统和智能体策略在训练过程中出现错误。因此在实际的机器人上进行所有强化学习训练是不现实的\cite{toussaint2018differentiable, todorov2012mujoco}。为了避免这个问题，可以使用物理仿真引擎对机器人和环境进行建模，并在实际测试智能体之前先在仿真环境中对智能体进行训练。幸运的是，随着强化学习仿真需求的增加，越来越多的针对机器人的仿真环境开始出现，在这些仿真环境中，可以像在真实环境中一样控制机器人的关节、调节各种参数，或获得传感器数据等等，并可以做到在真实环境中难以做到的设定复杂的稀疏奖励、获取碰撞次数、和改变环境的物理参数等操作\cite{savva2019habitat}。

        虽然已经有大量的软件系统可以用于在一个环境建模完全精确的情形下解决一个良定义的任务\cite{toussaint2018differentiable}，如何让智能体在面对未知的新环境和未知的新任务后能够有效泛化之前学习到的策略仍然是一个未完全解决的问题\cite{DBLP:journals/corr/abs-1803-11347,DBLP:journals/corr/abs-1905-04819}。人类可以在陌生的环境中用很少次数的探索自然地掌握大量有效信息，还可以利用已有经验对大量物体进行分类、提高对物理实体运动的预测能力，或创新性地设计工具解决问题。由于人类大脑的思维活动难以进行定量研究，这个过程通常很难被数值化为一个单一的奖励函数或强化学习算法。

        本课题致力于解决上述问题，即设计算法从而可以训练出能在任务奖励未知的环境中进行探索，获取环境信息，学习基本策略，并在任务确定后快速调节旧策略以适应新任务的智能体。为了实现这个目标，需要利用现有的开源物理仿真引擎、前沿的强化学习算法和具有强大函数拟合能力的深度神经网络来设计新算法。
        
        \subsection{研究意义}

        机器人控制对于工业制造有着重要的意义，在工厂流水线上，机械臂常常被设计为只能完成单个简单任务，或需要工人远程控制。虽然它们已经极大地提高了生产效率，减少了对工人们生命财产安全的威胁，但是机械臂由于价格昂贵，仅仅用于单一任务会造成极大的资源浪费。

        本课题提出的算法有希望训练出可以在对未知任务奖励的环境进行充分探索之后，快速适应多种不同任务的机器人智能体，从而扩大现有强化学习算法的适用范围，解决更复杂的控制任务，增强机器人智能体泛化策略的能力。

        此外，本课题还可以加深对现有强化学习算法在机器人控制中应用价值的理解，可以通过机器人仿真和控制帮助提前发现在应用算法到实际机器人控制时可能出现的问题，可以通过设计和调整深度神经网络进一步了解不同结构的神经网络对智能体性能的影响。
    
    \section{国内外研究进展}

    国内外已经有了很多关于让机器人智能体使用工具和泛化已学习的策略到新任务的研究。其中基于引导视觉的工具使用、无监督兴趣导向的探索、模仿学习和自驱动的主动学习与本课题有关。
        \subsection{基于引导视觉预见的工具使用}
        引导视觉预见\cite{xie2019improvisation}可以使机械臂智能体从人类演示中学习并泛化学习到的能力以在不同的环境中使用工具。这种方法包含动作提案模型和预测模型。其中动作提案模型使用演示动作数据来训练一个自回归的长短时记忆网络模型来根据图像传感器拍摄到的图像数据生成动作。预测模型是基于卷积长短时记忆网络的\cite{shi2015convolutional}。预测模型被用于对物理实体的运动进行物理预测，并可以被用于筛选不能完成指定目标的动作序列。训练过程分为基于演示的模仿学习和利用握爪反射的随机自动训练。在测试过程中，指定的目标被定义为像素移动，预测模型输出的像素位置和真实像素位置的距离被用于评估动作好坏。在指定任务后，动作序列从动作提案模型中采样，并使用交叉熵方法结合预测模型进行优化。实验表明，此种方法可以比单纯模仿学习在新环境中获得更好的泛化性能。

        \subsection{无监督兴趣导向探索}
        引导视觉预见需要有人类使用工具的演示数据才能正常工作，而对于更一般的环境，人类的演示数据可能是无法获得的，此时智能体应当可以在没有指定任务的情况下在环境中探索。为了解决这个问题，一个兴趣导向的探索方法被提出了\cite{laversanne-finot2018curiosity}。这个方法结合了解纠缠目标空间的变分自编码机（VAE）\cite{conf/nips/PuGHYLSC16}和兴趣导向的IMGEP（Intrinsically Motivated Goal Exploration Process）方法\cite{DBLP:journals/corr/abs-1708-02190}。在目标空间被给定后，IMGEP框架下的智能体会倾向于选择更有可能增加竞争力的目标。因为不像通常的强化学习算法一样在给定单一目标后做训练，而是无监督地选择目标进行训练，因此这是一个元策略算法。在探索过程中，智能体会学习到被$\beta$-VAE解纠缠后的观测，因此智能体可以分离地探索不同的物体。在实验中，此种方法获得了比单纯IMGEP方法更大的探索率。

        \subsection{模仿学习}
        在需要使用工具求解的开放任务中，往往有着稀疏奖励，随机探索很难刚好完成一个完整的动作序列，最终成功地使用工具完成任务，并获得奖励。
        模仿学习通过人类的专家策略，可以极大地加速这种随机探索过程。通过人类提供的演示数据，智能体应当能够学习到更好的探索策略，增大获得奖励的概率。
        相关研究表明，使用常规的强化学习算法和运动剪辑数据集，智能体可以学会组合学到的不同的技能，并用于求解多种任务\cite{peng2018deepmimic}。不仅如此，智能体也可以从视频中学习到一些技能\cite{peng2019sfv}。这意味着现有模仿学习方法可以利用网络中大量的视频数据进行学习。

        \subsection{自驱动的主动学习}
        在开放任务求解中，主动学习技术也可以被使用\cite{ren2020survey,conf/icml/EpshteynVD08,conf/nips/HoE16,conf/nips/DuanASHSSAZ17}。为了在主动学习过程中获得预测物理环境的能力，可以使用带策略的循环Q网络来减少未知物理性质的熵\cite{li2019active}。
        在机器人学中，逆动力学模型对于稳健控制非常重要。一种主动学习方法使用竞争力来选择目标并在高维连续空间中学习各种技能，并用于机器人控制\cite{baranes2013active}。仿真结果表明这种方法能够帮助机器人智能体探索到随机策略难以探索到的区域。

    \section{研究内容和方法}

        \subsection{研究内容}
        在开放任务中，时刻$t$下会有一个稀疏奖励$R_t\in\{0,-1\}$被提供给智能体。奖励为-1表示任务失败，奖励为0表示任务成功。
        该奖励是关于环境状态的函数，即$reward:\mathcal S\to\{0,-1\}$，其中$\mathcal S$表示状态空间。
        智能体被允许在$t< T_{start}$时间内对环境进行探索，但是无法获知函数$reward$或由此函数计算得到的稀疏奖励$R_t$，只能通过它观测到的状态向量$s\in\mathcal S$来计算内部奖励（intrinsic reward），根据此奖励对环境进行探索，并学习能用于被$reward$函数表示的任务有关的技能。
        在$t\geq T_{start}$时，在每一时刻$t$对智能体提供由函数$reward$计算得到的稀疏奖励$R_t=reward(s)$。
        理想情况下，在$t<T_{start}$时使用特定策略$\pi$进行探索的智能体应当能比使用随机选择动作的策略$\pi‘$进行探索的智能体更快地收敛到更高的奖励，即$\mathbb E_\pi[R_t]>\mathbb E_{\pi'}[R_t]$。

        \subsection{研究方法}
        本文的实验环境是基于Gym\cite{brockman2016openai}和Pyrobolearn\cite{delhaisse2019pyrobolearn}搭建的，其中Gym用于初步验证算法的正确性，大部分仿真实验在Pyrobolearn中完成。实验中对神经网络的优化使用了Pytorch\cite{paszke2019pytorch}，算法主体程序使用了Python语言。

        本文的研究方法是基于内部奖励的强化学习。其中内部奖励被用于鼓励智能体在未知环境中探索，并掌握可用于解决开放任务的技能，它与任务特定的稀疏奖励$reward$无关。

        在本文的研究中，内部奖励与事后经验重放（Hindsight Experience Replay）被结合起来，用于获得更丰富的探索奖励。其中，多种内部奖励方法被使用，如基于局部敏感哈希的计数奖励、基于正向动力学预测的奖励和本文提出的物理仿真时间奖励。
        本文中的强化学习架构采用传统的演员－评论家（Actor-Critic）模式\cite{konda2002actor}，其中演员网络和评论家网络都使用了多层感知机。
        为了加速算法的收敛，减小训练过程中的不稳定性，训练算法借用了TD3算法\cite{DBLP:journals/corr/abs-1802-09477}的结构，引入了两个评论家，设计了靶网络以减缓演员网络和评论家网络参数更新的速度，并提出了混合高斯噪声层用于防止确定性策略收敛到局部极小。

