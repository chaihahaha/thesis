% !Mode:: "TeX:UTF-8"

\hitsetup{
  %******************************
  % 注意：
  %   1. 配置里面不要出现空行
  %   2. 不需要的配置信息可以删除
  %******************************
  %
  %=====
  % 秘级
  %=====
  statesecrets={公开},
  natclassifiedindex={TM301.2},
  intclassifiedindex={62-5},
  %
  %=========
  % 中文信息
  %=========
  ctitlecover={基于虚拟物理仿真思考的开放任务求解},%放在封面中使用，自由断行
  ctitle={基于虚拟物理仿真思考的开放任务求解},%放在原创性声明中使用
  cxueke={工程},
  csubject={软件工程},
  caffil={计算学部},
  cauthor={柴士童},
  csupervisor={范晓鹏},
  % 日期自动使用当前时间，若需指定按如下方式修改：
  cdate={2020年12月},
  cstudentid={18S103239},
  cstudenttype={全日制工程硕士}, %非全日制教育申请学位者
  %cnumber={no9527}, %编号
  %cpositionname={哈铁西站}, %博士后站名称
  %cfinishdate={20XX年X月---20XX年X月}, %到站日期
  %csubmitdate={20XX年X月}, %出站日期
  %cstartdate={3050年9月10日}, %到站日期
  %cenddate={3090年10月10日}, %出站日期
  %（同等学力人员）、（工程硕士）、（工商管理硕士）、
  %（高级管理人员工商管理硕士）、（公共管理硕士）、（中职教师）、（高校教师）等
  %
  %
  %=========
  % 英文信息
  %=========
  etitle={Physical Simulation and Reasoning based Task-Agnostic learning},
  exueke={Technology},
  esubject={Software Engineering},
  eaffil={\emultiline[t]{Faculty of Computing}},
  eauthor={CHAI Shitong},
  esupervisor={Professor FAN Xiaopeng},
  %eassosupervisor={XXX},
  % 日期自动生成，若需指定按如下方式修改：
  edate={December, 2020},
  estudenttype={Master of Engineering},
  %
  % 关键词用“英文逗号”分割
  ckeywords={元学习, 强化学习, 机器人学, 物理仿真, 深度学习},
  ekeywords={Meta Learning, Reinforcement Learning, Robotics, Physics Simulation, Deep Learning},
}

\begin{cabstract}
本文在Pyrobolearn机器人仿真环境中设计了一个要求机械臂的末端执行器到达指定物体附近的开放任务，并设计了新的强化学习算法用于训练智能体在未获得任务相关的奖励之前在未知环境中学习到可泛化到该开放任务的策略。

在TD3算法和HER算法的基础上，本文引入了基于局部敏感哈希和计数的奖励和基于正向动力学预测的奖励用于鼓励智能体探索未知环境。


本文提出的斯混合噪声层被用于提供自适应的策略噪声。提出的基于物理仿真引擎仿真时间的奖励被用于在未知任务的环境中鼓励智能体学习有效可泛化的策略，并在获得稀疏奖励的开放任务后快速适应到新的策略。

\end{cabstract}

\begin{eabstract}
A task which requires the end effector of a manipulator to reach a specific body is designed with the help of the robot learning framework Pyrobolearn. New reinforcement learning algorithms are proposed to train an agent to learn in a task-agnostic environment without reward related to the task, where the learnt policy of the agent should be generalizable to the proposed task.

Based on TD3 and HER algorithms, a locality sensitive hashing based counting reward and a forward dynamics prediction model based reward are introduced to encourage the agent to explore in the unknown environment. 

    A mixed gaussian noise layer is proposed to provide a adaptive policy noise. A reward based on physics simulation time is proposed to encourage the agent to learn a generalizable policy in a task agnostic environment, and adapt to a new policy after given a task setting with sparse reward.
\end{eabstract}
